<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Linux on BLOG</title><link>https://wangloo.github.io/tags/linux/</link><description>Recent content in Linux on BLOG</description><generator>Hugo -- gohugo.io</generator><lastBuildDate>Sun, 18 Aug 2024 10:51:49 +0800</lastBuildDate><atom:link href="https://wangloo.github.io/tags/linux/index.xml" rel="self" type="application/rss+xml"/><item><title>Linux Rootfs 二级加载</title><link>https://wangloo.github.io/posts/os/linux/rootfs/</link><pubDate>Sun, 18 Aug 2024 10:51:49 +0800</pubDate><guid>https://wangloo.github.io/posts/os/linux/rootfs/</guid><description>一些常用名次的概念区分：
ramdisk：使用内存模拟的特殊的块设备，像是 EMMC、UFS 这种 ramfs、tmpfs：文件系统格式，像是 EXT4、F2FS 这种 initrd：init ramdisk，一个启动阶段专用的 ramdisk，存放第一级“临时 rootfs” initramfs：基于 tmpfs 的、专门用于启动阶段，同样存放第一级“临时 rootfs” rootfs：不是一种文件系统格式，而是一堆文件的统称。系统启动后，指那些真正的用户文件和系统程序，一般来说 rootfs 使用的文件系统可能是 EXT4 或 f2fs，底层的块设备是 EMMC 或 UFS。 Rootfs 定义和存在的问题 Linux 启动后用到的文件都存储在“根文件系统”中，也叫 rootfs，里面存放着所有系统程序和用户文件。因此，rootfs 一般是存储在块设备中，比如 UFS、EMMC、SCSI 等。
所以说，要想访问这些文件，执行这些系统程序需要先 初始化块设备的驱动程序，我理解这个过程是很慢的，相当于所有的任务都会被这个驱动初始化给 delay。
消耗的时间包括：块设备驱动初始化+文件系统格式初始化 我们可能会有一些任务比如说展示开机动画这些，本身是不依赖块设备，但却都堵塞在这。 为了解决这个问题，先后有两种策略：
initrd(based on ramdisk) initramfs(based on tmpfs) “initrd” based on “Ramdisk” initrd 首先被提出以解决上述问题，initrd=init ramdisk，本质上属于一个 ramdisk 设备，所以这里需要先对 ramdisk 进行介绍。
ramdisk 是一个用内存模拟的块设备，就和 EMMC、UFS（这俩都属于 SCSI）类似。既然作为块设备，在它之上需要一个文件系统格式（例如 ext4、f2fs）才能使用。 ramdisk 可能有其他的用处，ramdisk 的使用 不需要那么复杂的设备驱动程序。 initrd 的使用方法是：在 Linux 加载时，除了内核和设备树之外，同时加载了一个带有文件系统格式的镜像到内存中（例如.</description></item><item><title>Linux 内核开发经验技巧</title><link>https://wangloo.github.io/posts/os/linux/devel_kern/</link><pubDate>Fri, 17 May 2024 14:51:49 +0800</pubDate><guid>https://wangloo.github.io/posts/os/linux/devel_kern/</guid><description>查看内核的符号
cat /proc/kallsyms | grep tracepoint 查看内核启动log
dmesg | more 运行时查看内核的设备输
在 /sys/fireware 下是关于Linux内核运行时的设备信息，其中fdt是dtb格式。而devicetree/ 按照层级目录的形式展现dts。
查看内核的.config
# zcat 快速预览压缩文件内容 zcat /proc/config.gz</description></item><item><title>Linux cmdline 配置</title><link>https://wangloo.github.io/posts/os/linux/cmdline/</link><pubDate>Tue, 16 Apr 2024 10:30:35 +0800</pubDate><guid>https://wangloo.github.io/posts/os/linux/cmdline/</guid><description>内核启动时会打印当前生效的 cmdline。Command Line 相当于外部传给 Linux 内核的参数，内核针对他们做相应处理，并打印无法是被的参数。
如何看当前的cmdline：
内核启动日志会输出 [ 0.000000] Kernel command line: earlycon rw rdinit=/linuxrc root=/dev/vda nokaslr [ 0.000000] Unknown command line parameters: nokaslr cat /proc/cmdline/ 用户有几种方式来注入cmdline：
设备树 bootargs。 linuxkernel的设备树是QEMU生成的，实际就是用的启动参数 --append。
qemu-system-aarch64 \ -nographic -machine virt,secure=on \ -cpu cortex-a53 -smp 2 -m 4G \ -d guest_errors,unimp \ -gdb tcp::1234 \ -bios ./arm-trusted-firmware/build/qemu/debug/bl1.bin \ -kernel ./wupeng/linux/arch/arm64/boot/Image \ -initrd .</description></item><item><title>Linux Buddy 内存分配器</title><link>https://wangloo.github.io/posts/os/linux/mem/buddy/</link><pubDate>Mon, 18 Sep 2023 17:51:49 +0800</pubDate><guid>https://wangloo.github.io/posts/os/linux/mem/buddy/</guid><description>伙伴系统的优势 作为一个页分配器，伙伴系统主要解决外部碎片过多的问题， 保证系统中尽可能有大的连续空间可以使用。
这也正是伙伴系统要设计成相邻内存块合并的原因。</description></item><item><title>Linux 进程间通信概述</title><link>https://wangloo.github.io/posts/os/linux/ipc/linux-ipc/</link><pubDate>Fri, 08 Sep 2023 16:21:27 +0800</pubDate><guid>https://wangloo.github.io/posts/os/linux/ipc/linux-ipc/</guid><description>SystemV IPC Linux 引入了 SystemV 中 IPC 的集中实现方式，包括：信号量、共享内存、消息队列。
共享内存 共享内存基于文件实现，用操作文件的方式来操作共享内存区。
原理是对一块物理内存做多个映射，用引用计数来维护，只有引用计数为0时，才能释放。
共享内存的特点是：
速度快，但自身没有同步功能，需要配合外部的同步机制。 信号量 为什么说信号量也是一种通信机制?
其实通信并不一定就是要发送数据，只要能够相互感知，通知到对方，就算是一种通信。 类比抛媚眼也算是通信的一种。
消息队列 并非基于文件，由自己的一套API，使用起来不方便。 消息队列是面向消息的（并非字节流），消息由类型。 消息队列有自己的同步机制，无需外部添加。 信号 常用于父子之间通信，只要你知道了对方的PID，就可以给对方发信号。
用kill(pid, signal)来发送信号。</description></item><item><title>Linux SLAB 内存分配器(3): SLUB/SLOB</title><link>https://wangloo.github.io/posts/os/linux/mem/slab3/</link><pubDate>Fri, 26 May 2023 18:51:49 +0800</pubDate><guid>https://wangloo.github.io/posts/os/linux/mem/slab3/</guid><description>slub 和 slob 是基于 slab 思想针对某些场景下的优化实现。
SLUB 当 slab 分配器面对过多的申请需求时，cache 中就会有多个 slab (struct slab), 在以前的 slab 分配器设计中， slab 描述符是放在物理页中的，即物理页的结构为： （slab 描述符+freelist+对象 s）,管理数据结构的开销就比较大。后期 SLUB 首先将 slab 描述符与struct page共用（通过 union 实现）。后面该思想被 SLAB 采纳。 SLAB 中每个 cache node 有三个 list: free, partial, full， 管理起来很麻烦， SLUB 中只有一个 partial 链表。 放弃着色，效果不明显 SLOB SLOB 的设计更加简洁，只有 600 行左右代码（SLAB，SLUB 都是 4000+），适合小内存的嵌入式设备。
SLOB 中没有对象的概念，每个 slab 中分配的小块内存大小可以是不同的， 通过长度+偏移来记录下一个小块内存的位置。
另外，SLOB 基本上放弃了 cache 的思想，系统中通过创建三个全局的链表: small, medium, large, 分别应对&amp;lt;256b, &amp;lt;1k, &amp;lt;PAGESIZE 的请求， slab 直接挂在这三个链表上，因为 slab 中的内存分配大小可以不同， 用三个链表可以加速查找。</description></item><item><title>Linux SLAB 内存分配器(2): 算法</title><link>https://wangloo.github.io/posts/os/linux/mem/slab2/</link><pubDate>Sat, 20 May 2023 18:51:49 +0800</pubDate><guid>https://wangloo.github.io/posts/os/linux/mem/slab2/</guid><description>上一篇介绍了数据结构，这一篇主要介绍 slab 分配器的分配和释放算法。
最外层接口: kmalloc()/kfree() 最上层的接口是kmalloc(size, flag)。
slab 分配器维护了多个不同大小的 kmem_cache，放在数组kmem_caches[]中, 其对应的 object 大小和该 kmem_cache 的 name 在另一个数组kmalloc_info[] 中，它们的下标是对应的。使得我们能根据请求分配的大小来找到对应的struct kmem_cache结构。 【代码】
专用的&amp;quot;cache&amp;quot; 上面的结构，会遍历系统初始化创建的一些内存池，来寻找一个大小满足要求的 object， 但是通常不能找到大小相等的，如果系统中存在的固定 cache 中 object 的大小太稀疏， 就容易发生空间浪费的问题。
因此，我们可以为某个特定大小的内存请求再创建一个单独的 cache，仅仅用于满足这一类 结构体的申请，也是符合 slab 分配器关于面向对象的设计思想。
slab 分配器提供的相关接口是:
kmem_cache_create(): 创建一个专用 cache kmem_cache_alloc()： 从指定的 cache 里分配 object kmem_cache_free(): 释放对象到指定的 cache kmem_cache_destory(): 销毁某个 cache Reference https://blog.csdn.net/u010923083/article/details/116518646?spm=1001.2014.3001.5502</description></item><item><title>Linux SLAB 内存分配器(1): 概述</title><link>https://wangloo.github.io/posts/os/linux/mem/slab1/</link><pubDate>Sat, 20 May 2023 17:51:49 +0800</pubDate><guid>https://wangloo.github.io/posts/os/linux/mem/slab1/</guid><description>参考的 linux kernel 代码版本 4.12
slab 是什么 slab 属于 linux 内核内存分配器的一种，满足细粒度的小块内存的请求。 内核中还有其他的内存分配器例如伙伴系统，它是满足页为单位的分配请求。 因为内核中大部分的分配请求都用不到一个页那么大，所以 slab 的出现能够减小 内存碎片的出现。
另外，非常重要的是，除了基本的小块内存分配， slab 的最初设计开始就基于 对象缓存的思想，加速分配和初始化的过程，下面将详细介绍缓存的设计思想。
slab 分配器的实现在 linux 中是基于伙伴系统的，slab 管理的内存来源 就是伙伴系统，只是进行“二次管理”， 。
slab 的设计思想 对象缓存特性 经常会在 slab 接口中看到kmem_cache这个前缀，我最初也有疑问说 slab 不就是一个内存分配算法，和 cache 扯上什么关系呢？
slab 一般用于分配一些结构的内存，拿struct task来举例，我们通常会为 struct task创建一个内存池，里面包含了若干大小为sizeof(struct task) 的内存块，用的时候从里面取，释放之后回归池子里即可。这是 slab 分配小块内存的 基本思想。
内核中的很多数据结构，我们在申请完空间之后立马做的一件事，就是初始化对象的成员 为某些特定的值，可以称这个过程为结构体(类)的构造函数，意为所有对象都会 做的那些相同的事。比如说，多核环境下很多结构中会有锁，或者链表，那么申请完空间 之后都会做锁或链表做初始化，这是固定的。实际上这些操作消耗的时间甚至大于申请 一块内存。
基于以上事实，slab 分配器做的缓存优化是：为每个类别的内存池都绑定一个构造函数 和析构函数，当用完的对象空间被释放时，调用析构函数将某些成员的值恢复为默认状态 ，这样下次申请的时候，直接拿就行了，省略了重复的初始化流程。而构造函数被调用的 情况仅仅是当该小块内存第一次被申请时。
由于这个思想，整个内存池也就被声明结构 struct kmem_cache, 它是整个 slab 算法的顶层数据结构，其中包含了许多相同大小的小内存块，slab 通过一些算法对其进行 管理。
整体数据结构的规划 上面说了整个系统的顶层结构是struct kmem_cache, 其中可以再划分为多个&amp;quot;slab&amp;quot;, 这个 slab 就能代表一个或多个连续的物理页嘛，从 buddy 申请来的。</description></item><item><title>Linux 内核数据结构 hlist</title><link>https://wangloo.github.io/posts/os/linux/data_struct/hlist/</link><pubDate>Thu, 11 May 2023 20:51:49 +0800</pubDate><guid>https://wangloo.github.io/posts/os/linux/data_struct/hlist/</guid><description>linux 内核为创建【用单链表解决冲突的哈希表】设计了专门的数据结构 hlist。
hlist 整体来说是带头结点的双向链表，头结点的类型为hlist_head, 普通节点 的类型为hlist_node. 为什么要区别两种类型？节约空间， 因为哈希表的 表项类型可以是hlist_head, 它其实不需要prev指针, 比起一般的结点，一个 哈希表能节约一半的空间。
所以一个哈希表和头结点的结构可表示为:
struct hlist_head { struct hlist_node *first; }; struct hlist_head table[TALBE_SZ]; 二象性 任何事物都具有二象性，区分两种类型节约空间的空间，也带了一个问题： 首个hlist_node结点的prev指向哪呢？
正常情况下肯定毫不犹豫的指向头结点，即hlist_head，但注意此时类型是 不同的，prev不能同时是struct hlist_head*和struct hlist_node *。
解决方案有两个，首先可以使首个结点的prev=NULL, 这样虽然避免了类型引发的 问题，也能保证功能正确，但是却破坏了一致性，使得操作的复杂度上升，增加了许多 判断分支。
// delelt a node void del_node(struct hlist_head *head, struct hlist_node *node) { // 这个if 本来是不需要的，甚至参数的head 也不需要传， // 更好的处理方式见解决方案2 if (node == head-&amp;gt;first) { head-&amp;gt;first = node-&amp;gt;next; } else { node-&amp;gt;prev-&amp;gt;next = node-&amp;gt;next; } if (node-&amp;gt;next) { node-&amp;gt;next-&amp;gt;prev = node-&amp;gt;prev; } } // insert a node void add_node_before(struct hlist_head *head, struct hlist_node *new struct hlist_node *next) { // 这个if 本来是不需要的，参数head也是不需要传递的 if (next == head-&amp;gt;first) { new-&amp;gt;prev = NULL; head-&amp;gt;first = new; } else { new-&amp;gt;prev = next-&amp;gt;prev; new-&amp;gt;prev-&amp;gt;next = new; } new-&amp;gt;next = next; next-&amp;gt;prev = new; 更好的解决方案: **prev 改变struct hlist_node的构成，使用二级指针:</description></item><item><title>Linux Trace(1): Tracepoint</title><link>https://wangloo.github.io/posts/os/linux/trace/tracepoint/</link><pubDate>Sun, 23 Apr 2023 23:51:49 +0800</pubDate><guid>https://wangloo.github.io/posts/os/linux/trace/tracepoint/</guid><description>tracepoint 是 Linux trace system 中 data source 之一， 其 trace 的对象是 kernel，属于一种静态的插桩方法。
添加和删除需要手动修改内核源码 可以向上提供接口，可以通过 frontend 来开启或者关闭，也可以自定义数据处理方式 在 disable 时， 仅有一次 if 判断的损耗，所以效率还算高。但缺点是不够灵活。 tracepoint 的组成 看其源码struct tracepoint就能知道它的组成结构：
struct tracepoint { const char *name; #define TP_STATE_DISABLE 0 #define TP_STATE_ENABLE 1 int state; // 并非用于注册hook的函数，而是注册hook时的hook int (*reghook)(void); void (*unreghook)(void); // 在tracepoint触发时将调用的hook struct tracepoint_hook *hooks; }; name: 是该 tracepoint 的名称 state: 用于控制其开关状态 hooks: 是一系列的函数指针，当 tracepoint hit 时，这些函数会被依次调用 reghook/unreghook: 在注册/注销 hook 时将被调用，可以用来输出一些提示信息 为了提供对 tracepoint 操作的接口，定义一个 tracepoint 时，会同时定义一系列功能函数, 包括：</description></item><item><title>Linux 内核抢占</title><link>https://wangloo.github.io/posts/os/linux/schedule/kernel_preempt/</link><pubDate>Thu, 13 Apr 2023 23:51:49 +0800</pubDate><guid>https://wangloo.github.io/posts/os/linux/schedule/kernel_preempt/</guid><description>抢占的含义 抢占指的是强制使一个任务让出 CPU 给其他任务。
抢占是调度器做的，每次执行schedule()就可能发生一次抢占，所以 抢占发生的地点是内核，也就是schedule()的执行环境。
用户抢占 与内核抢占相对应的是用户抢占，用户抢占不是指抢占发生的地点，因为 上面说了抢占发生的地点一定是内核。
所以用户抢占的含义是：抢占的时机是用户态，换句话说就是抢占发生之前， 系统正处于用户态。
用户抢占的经典场景是时钟中断，用户进程 1 执行的好好地，被时钟中断打断 然后中断返回时执行重调度，选择了新的用户进程 2。其他的可能用户抢占的场景 还有系统调用返回时， 总之是内核返回用户态时都会发生用户抢占。
内核抢占 启用内核抢占增加了系统中发生抢占的点，即抢占前系统正处于内核。
当一个进程正处于内核态执行任务时，比如执行mmap()系统调用的任务，在 未开启内核抢占的情况下，中断返回时只可能继续执行当前进程的任务，不会 发生调度。
当启用内核抢占时，上述情况下若发生中断，系统在退出中断后，即使此时不是 返回用户态，也可以执行schedule()，即可以发生抢占。此之谓内核抢占。
抢占发生的条件 启用内核抢占之后，其实抢占的过程也不区分用户态和内核态，只要满足条件都会 执行schedule()。
执行重调度的条件有两个:
是否需要重调度? 是否可以重调度? 是否需要重调度也就是何时执行schedule()的问题，大概包含以下的场景:
时钟中断 新进程创建 修改进程的 nice 值 中断返回内核态 内核恢复为可抢占(下面会介绍) 然而有一些情况不可以重新调度，比如内核中一些关键的步骤，那些不能被打断的 原子操作。
在关键步骤之前，需要调用preempt_disable()，此时 linux 会在 tcb 中会改变 preempt_count的值，这个操作不是关闭中断，而是在中断返回时即使有更高优先级的其他进程， 只要该值不符合要求，重调度也不会发生。
关键步骤执行完，调用preempt_enable()，此时为了去满足关键区域内可能 有新加入的高优先级进程，会调用一次重调度，这也正是上面所说需要重调度的场景之一。</description></item></channel></rss>